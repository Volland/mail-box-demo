## Export Task 

### Mapping of data and file 

I going to ignore UID field in a favor of autogenerated sequence value for id fields.
It is possible if needed to have extra field like **external_uid** 

### Toolset strategy 
The best code ever is the code that not exist.
Less code == less bugs 

It is possible to have export task quite mature with postgres copy command. 
Copy command is quite fast and optimised for bulk inserts. 

```postgresql
COPY public.messages (sender, subject, message, send_at) from stdin  with (format CSV )
```

The key challenge it is use CSV format by default. 

We could use a jq - a small and handy tool . swiss knife for json.

``` bash
 jq  -r '.messages[] | [.sender, .subject, .message, .time_sent] | @csv'
 
```

jq - really save me a lot of time and replace a bunch of helper scripts
 - -r - ask for raw strings 
 - .messages[] - select a root data element 
 - [.sender, .subject, .message, .time_sent]  - form array of picked values . define order 
 - @csv - csv filter ))
 
 Lets assemble all together 
 
 ```bash
 cat dump-data.json | jq  -r '.messages[] | [.sender, .subject, .message, .time_sent] | @csv' | psql -h localhost -p 5432 -d mailboxdev -U postgres -c "COPY public.messages (sender, subject, message, send_at) from stdin  with (format CSV )"

```
 
 ### Code 
 
 Still code could give you more control and apply robust rules. that maybe could be not covered by db schema.
 So Im using the same copy command with a streams.
 In a node js streams and pipes is powerful tool that move big part of heavy work to os.
 
 I decide to include it as a function to db-provider module.
 It is allow to reuse a same db pool and do a error handling and db work in a same place.
 
 One more note . for data validation I use a json scheme that allow to filter out values from file before we get to db.
 [exportMessagesFromJson](./db/db-provider.js) 
 